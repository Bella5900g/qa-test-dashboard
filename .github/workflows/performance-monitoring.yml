# QA Test Automation Dashboard - Performance Monitoring
# Desenvolvido por Isabella Barbosa - Engenheira de QA Sênior

name: 📊 Performance Monitoring

on:
  schedule:
    # Executar monitoramento de performance diariamente às 3h UTC
    - cron: '0 3 * * *'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Tipo de teste de performance'
        required: true
        default: 'load'
        type: choice
        options:
        - load
        - stress
        - spike
        - volume

jobs:
  # =============================================================================
  # Job: Testes de Performance Automatizados
  # =============================================================================
  performance-tests:
    name: ⚡ Testes de Performance
    runs-on: ubuntu-latest

    steps:
      - name: 📥 Checkout código
        uses: actions/checkout@v4

      - name: 🐍 Configurar Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: 📦 Instalar dependências
        run: |
          python -m pip install --upgrade pip
          pip install -r backend/requirements.txt
          pip install requests psutil

      - name: 🚀 Iniciar backend
        run: |
          cd backend
          python app.py &
          sleep 15

      - name: ⚡ Executar testes de performance
        run: |
          cd automation/performance
          python run_performance_tests.py

      - name: 📊 Analisar resultados
        run: |
          cd automation/performance/results
          
          # Extrair métricas dos resultados
          if [ -f "performance_results.jtl" ]; then
            echo "## 📊 Métricas de Performance" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            # Contar total de requests
            total_requests=$(tail -n +2 performance_results.jtl | wc -l)
            echo "**Total de Requests:** $total_requests" >> $GITHUB_STEP_SUMMARY
            
            # Calcular taxa de sucesso
            successful_requests=$(tail -n +2 performance_results.jtl | awk -F',' '$8=="true"' | wc -l)
            success_rate=$((successful_requests * 100 / total_requests))
            echo "**Taxa de Sucesso:** $success_rate%" >> $GITHUB_STEP_SUMMARY
            
            # Calcular tempo médio de resposta
            avg_response_time=$(tail -n +2 performance_results.jtl | awk -F',' '{sum+=$2; count++} END {print sum/count}')
            echo "**Tempo Médio de Resposta:** ${avg_response_time}ms" >> $GITHUB_STEP_SUMMARY
            
            # Calcular throughput
            throughput=$(echo "scale=2; $total_requests / 300" | bc)
            echo "**Throughput:** ${throughput} req/s" >> $GITHUB_STEP_SUMMARY
          fi

      - name: 📄 Upload relatórios de performance
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-reports-${{ github.run_number }}
          path: |
            automation/performance/results/
          retention-days: 30

  # =============================================================================
  # Job: Monitoramento de Métricas do Sistema
  # =============================================================================
  system-monitoring:
    name: 🖥️ Monitoramento do Sistema
    runs-on: ubuntu-latest

    steps:
      - name: 📥 Checkout código
        uses: actions/checkout@v4

      - name: 🐍 Configurar Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: 📦 Instalar dependências
        run: |
          python -m pip install --upgrade pip
          pip install psutil requests

      - name: 🚀 Iniciar backend
        run: |
          cd backend
          python app.py &
          sleep 10

      - name: 📊 Coletar métricas do sistema
        run: |
          python -c "
          import psutil
          import requests
          import json
          import time
          from datetime import datetime
          
          # Coletar métricas por 5 minutos
          metrics = []
          for i in range(30):  # 30 amostras de 10 segundos cada
              # Métricas do sistema
              cpu_percent = psutil.cpu_percent(interval=1)
              memory = psutil.virtual_memory()
              disk = psutil.disk_usage('/')
              
              # Métricas da aplicação
              try:
                  response = requests.get('http://localhost:5000/api/sistema', timeout=5)
                  app_metrics = response.json()
              except:
                  app_metrics = {'cpu': 0, 'memoria': 0, 'disco': 0, 'rede': 0}
              
              metric = {
                  'timestamp': datetime.now().isoformat(),
                  'system_cpu': cpu_percent,
                  'system_memory': memory.percent,
                  'system_disk': (disk.used / disk.total) * 100,
                  'app_cpu': app_metrics.get('cpu', 0),
                  'app_memory': app_metrics.get('memoria', 0),
                  'app_disk': app_metrics.get('disco', 0),
                  'app_network': app_metrics.get('rede', 0)
              }
              
              metrics.append(metric)
              time.sleep(10)
          
          # Salvar métricas
          with open('system_metrics.json', 'w') as f:
              json.dump(metrics, f, indent=2)
          
          print(f'✅ Coletadas {len(metrics)} amostras de métricas')
          "

      - name: 📊 Analisar métricas coletadas
        run: |
          python -c "
          import json
          import statistics
          
          with open('system_metrics.json', 'r') as f:
              metrics = json.load(f)
          
          if metrics:
              # Calcular estatísticas
              system_cpu = [m['system_cpu'] for m in metrics]
              system_memory = [m['system_memory'] for m in metrics]
              app_cpu = [m['app_cpu'] for m in metrics]
              app_memory = [m['app_memory'] for m in metrics]
              
              print('## 🖥️ Métricas do Sistema')
              print('')
              print(f'**CPU do Sistema:**')
              print(f'- Média: {statistics.mean(system_cpu):.2f}%')
              print(f'- Máximo: {max(system_cpu):.2f}%')
              print(f'- Mínimo: {min(system_cpu):.2f}%')
              print('')
              print(f'**Memória do Sistema:**')
              print(f'- Média: {statistics.mean(system_memory):.2f}%')
              print(f'- Máximo: {max(system_memory):.2f}%')
              print(f'- Mínimo: {min(system_memory):.2f}%')
              print('')
              print(f'**CPU da Aplicação:**')
              print(f'- Média: {statistics.mean(app_cpu):.2f}%')
              print(f'- Máximo: {max(app_cpu):.2f}%')
              print(f'- Mínimo: {min(app_cpu):.2f}%')
              print('')
              print(f'**Memória da Aplicação:**')
              print(f'- Média: {statistics.mean(app_memory):.2f}%')
              print(f'- Máximo: {max(app_memory):.2f}%')
              print(f'- Mínimo: {min(app_memory):.2f}%')
          "

      - name: 📄 Upload métricas do sistema
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: system-metrics-${{ github.run_number }}
          path: system_metrics.json
          retention-days: 30

  # =============================================================================
  # Job: Análise de Tendências
  # =============================================================================
  trend-analysis:
    name: 📈 Análise de Tendências
    runs-on: ubuntu-latest
    needs: [performance-tests, system-monitoring]

    steps:
      - name: 📥 Checkout código
        uses: actions/checkout@v4

      - name: 📊 Download artefatos anteriores
        uses: actions/download-artifact@v4
        with:
          pattern: performance-reports-*
          path: ./previous-reports
          merge-multiple: true

      - name: 📈 Analisar tendências
        run: |
          echo "## 📈 Análise de Tendências" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### 📊 Comparação com Execuções Anteriores" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Contar execuções anteriores
          previous_count=$(find ./previous-reports -name "*.jtl" 2>/dev/null | wc -l)
          echo "**Execuções Anteriores Analisadas:** $previous_count" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ $previous_count -gt 0 ]; then
            echo "✅ Dados históricos disponíveis para análise de tendências" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### 🎯 Recomendações:" >> $GITHUB_STEP_SUMMARY
            echo "- Monitorar tendências de degradação de performance" >> $GITHUB_STEP_SUMMARY
            echo "- Alertar se tempo de resposta aumentar > 20%" >> $GITHUB_STEP_SUMMARY
            echo "- Investigar picos de uso de CPU/memória" >> $GITHUB_STEP_SUMMARY
          else
            echo "⚠️ Dados históricos insuficientes para análise de tendências" >> $GITHUB_STEP_SUMMARY
            echo "Execute mais execuções para estabelecer baseline" >> $GITHUB_STEP_SUMMARY
          fi

  # =============================================================================
  # Job: Alertas de Performance
  # =============================================================================
  performance-alerts:
    name: 🚨 Alertas de Performance
    runs-on: ubuntu-latest
    needs: [performance-tests, system-monitoring]
    if: always()

    steps:
      - name: 📊 Verificar thresholds de performance
        run: |
          echo "## 🚨 Verificação de Alertas" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Verificar se há relatórios de performance
          if [ -f "automation/performance/results/performance_results.jtl" ]; then
            # Extrair métricas críticas
            total_requests=$(tail -n +2 automation/performance/results/performance_results.jtl | wc -l)
            successful_requests=$(tail -n +2 automation/performance/results/performance_results.jtl | awk -F',' '$8=="true"' | wc -l)
            success_rate=$((successful_requests * 100 / total_requests))
            
            # Calcular tempo médio de resposta
            avg_response_time=$(tail -n +2 automation/performance/results/performance_results.jtl | awk -F',' '{sum+=$2; count++} END {print sum/count}')
            
            echo "### 📊 Métricas Atuais:" >> $GITHUB_STEP_SUMMARY
            echo "- **Taxa de Sucesso:** $success_rate%" >> $GITHUB_STEP_SUMMARY
            echo "- **Tempo Médio de Resposta:** ${avg_response_time}ms" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            # Verificar alertas
            alerts=0
            
            if [ $success_rate -lt 95 ]; then
              echo "🚨 **ALERTA:** Taxa de sucesso abaixo de 95% ($success_rate%)" >> $GITHUB_STEP_SUMMARY
              alerts=$((alerts + 1))
            fi
            
            if (( $(echo "$avg_response_time > 2000" | bc -l) )); then
              echo "🚨 **ALERTA:** Tempo de resposta acima de 2s (${avg_response_time}ms)" >> $GITHUB_STEP_SUMMARY
              alerts=$((alerts + 1))
            fi
            
            if [ $alerts -eq 0 ]; then
              echo "✅ **Status:** Todas as métricas dentro dos limites aceitáveis" >> $GITHUB_STEP_SUMMARY
            else
              echo "⚠️ **Total de Alertas:** $alerts" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "⚠️ Relatórios de performance não encontrados" >> $GITHUB_STEP_SUMMARY
          fi

      - name: 📢 Notificar alertas críticos
        uses: 8398a7/action-slack@v3
        if: failure()
        with:
          status: failure
          channel: '#performance-alerts'
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}
          fields: repo,message,commit,author,action,eventName,ref,workflow
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }}

  # =============================================================================
  # Job: Relatório Final
  # =============================================================================
  final-report:
    name: 📋 Relatório Final
    runs-on: ubuntu-latest
    needs: [performance-tests, system-monitoring, trend-analysis, performance-alerts]
    if: always()

    steps:
      - name: 📊 Gerar relatório final
        run: |
          echo "## 📋 Relatório de Monitoramento de Performance" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Data/Hora:** $(date -u +'%d/%m/%Y %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
          echo "**Execução:** ${{ github.run_number }}" >> $GITHUB_STEP_SUMMARY
          echo "**Commit:** ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### 📊 Status dos Jobs:" >> $GITHUB_STEP_SUMMARY
          echo "- ⚡ Testes de Performance: ${{ needs.performance-tests.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- 🖥️ Monitoramento do Sistema: ${{ needs.system-monitoring.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- 📈 Análise de Tendências: ${{ needs.trend-analysis.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- 🚨 Alertas de Performance: ${{ needs.performance-alerts.result }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### 🎯 Próximos Passos:" >> $GITHUB_STEP_SUMMARY
          echo "1. Revisar métricas coletadas" >> $GITHUB_STEP_SUMMARY
          echo "2. Investigar alertas se houver" >> $GITHUB_STEP_SUMMARY
          echo "3. Ajustar thresholds se necessário" >> $GITHUB_STEP_SUMMARY
          echo "4. Planejar otimizações baseadas nos dados" >> $GITHUB_STEP_SUMMARY
